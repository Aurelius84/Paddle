From 25e1de6d60b790d894b623d8c87fc7bd3bbdcd9f Mon Sep 17 00:00:00 2001
From: Aurelius84 <liujiezhangbupt@gmail.com>
Date: Thu, 14 Nov 2019 17:05:29 +0800
Subject: [PATCH 1/2] add fc grnn seq_fc

---
 paddle/fluid/operators/search_compute_2.h  | 356 ++++++++++++++
 paddle/fluid/operators/search_fc_op.cc     | 163 +++++++
 paddle/fluid/operators/search_grnn_op.cc   | 541 +++++++++++++++++++++
 paddle/fluid/operators/search_grnn_op.h    |  19 +
 paddle/fluid/operators/search_seq_fc_op.cc | 178 +++++++
 python/paddle/fluid/layers/__init__.py     |   3 +
 python/paddle/fluid/layers/search_nn.py    | 148 ++++++
 7 files changed, 1408 insertions(+)
 create mode 100644 paddle/fluid/operators/search_compute_2.h
 create mode 100644 paddle/fluid/operators/search_fc_op.cc
 create mode 100644 paddle/fluid/operators/search_grnn_op.cc
 create mode 100644 paddle/fluid/operators/search_grnn_op.h
 create mode 100644 paddle/fluid/operators/search_seq_fc_op.cc
 create mode 100644 python/paddle/fluid/layers/search_nn.py

diff --git a/paddle/fluid/operators/search_compute_2.h b/paddle/fluid/operators/search_compute_2.h
new file mode 100644
index 0000000000..17e6ce4ec3
--- /dev/null
+++ b/paddle/fluid/operators/search_compute_2.h
@@ -0,0 +1,356 @@
+/* Copyright (c) 2018 PaddlePaddle Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License. */
+#pragma once
+#include <immintrin.h>  // sse
+#include <cfloat>
+#include <cmath>    //fabs
+#include <cstring>  // memcpy
+#include "paddle/fluid/operators/math/blas.h"
+#include "paddle/fluid/operators/math/math_function.h"
+//#include "naive_gemm.h"
+namespace paddle {
+    namespace operators {
+        using Tensor = framework::Tensor;
+        using LoDTensor = framework::LoDTensor;
+        using LoD = framework::LoD;
+        template <typename DeviceContext, typename T>
+        void call_gemm(const math::BlasT<DeviceContext, T>& blas,
+                       const CBLAS_TRANSPOSE TransA, const CBLAS_TRANSPOSE TransB,
+                       const int M, const int N, const int K, const T alpha, const T* A,
+                       const T* B, const T beta, T* C) {
+#ifndef __NAIVE_GEMM__
+            int lda = (TransA == CblasNoTrans) ? K : M;
+            int ldb = (TransB == CblasNoTrans) ? N : K;
+            blas.GEMM(TransA, TransB, M, N, K, alpha, A, lda, B, ldb, beta, C, N);
+#else
+            naive::gemm((TransA == CblasTrans), (TransB == CblasTrans), M, N, K, alpha, A,
+              B, beta, C);
+#endif  // !__NAIVE_GEMM__
+        }
+        template <typename T>
+        void call_gemm(const framework::ExecutionContext& ctx,
+                       const CBLAS_TRANSPOSE TransA, const CBLAS_TRANSPOSE TransB,
+                       const int M, const int N, const int K, const T alpha, const T* A,
+                       const T* B, const T beta, T* C) {
+#ifndef __NAIVE_GEMM__
+            int lda = (TransA == CblasNoTrans) ? K : M;
+            int ldb = (TransB == CblasNoTrans) ? N : K;
+            auto blas = math::GetBlas<platform::CPUDeviceContext, T>(ctx);
+            blas.GEMM(TransA, TransB, M, N, K, alpha, A, lda, B, ldb, beta, C, N);
+#else
+            naive::gemm((TransA == CblasTrans), (TransB == CblasTrans), M, N, K, alpha, A,
+              B, beta, C);
+#endif  // !__NAIVE_GEMM__
+        }
+        template <typename T>
+        void call_gemm_batched(const framework::ExecutionContext& ctx,
+                               const CBLAS_TRANSPOSE TransA,
+                               const CBLAS_TRANSPOSE TransB, const int M, const int N,
+                               const int K, const T alpha, const T** A,
+                               const T** B, const T beta, T** C,
+                               const int batch) {
+            for (int i = 0; i < batch; ++i) {
+                call_gemm(ctx, TransA, TransB, M, N, K, alpha, A[i], B[i], beta, C[i]);
+            }
+        }
+// To align with Lego
+#ifndef LEGO_USE_FLOAT
+#define LEGO_USE_FLOAT
+#endif
+#ifndef LEGO_SSE
+#define LEGO_SSE
+#endif
+#if defined(LEGO_USE_FLOAT)
+#define __m256x __m256
+#define __m128x __m128
+        static const unsigned int AVX_STEP_SIZE = 8;
+        static const unsigned int SSE_STEP_SIZE = 4;
+        static const unsigned int AVX_CUT_LEN_MASK = 7U;
+        static const unsigned int SSE_CUT_LEN_MASK = 3U;
+#define _mm256_setzero_px _mm256_setzero_ps
+#define _mm256_mul_px _mm256_mul_ps
+#define _mm256_add_px _mm256_add_ps
+#define _mm256_load_px _mm256_loadu_ps
+#define _mm256_hadd_px _mm256_hadd_ps
+#define _mm256_permute2f128_px _mm256_permute2f128_ps
+#define _mm256_store_px _mm256_storeu_ps
+#define _mm256_broadcast_sx _mm256_broadcast_ss
+#define _mm256_castpx256_px128 _mm256_castps256_ps128
+#define _mm256_max_px _mm256_max_ps
+#define _mm256_sub_px _mm256_sub_ps
+#define _mm256_set1_px _mm256_set1_ps
+#define _mm256_sqrt_px _mm256_sqrt_ps
+#define _mm256_div_px _mm256_div_ps
+#define _mm_setzero_px _mm_setzero_ps
+#define _mm_add_px _mm_add_ps
+#define _mm_mul_px _mm_mul_ps
+#define _mm_load_px _mm_loadu_ps
+#define _mm_hadd_px _mm_hadd_ps
+#define _mm_store_sx _mm_store_ss
+#define _mm_store_px _mm_storeu_ps
+#define _mm_load1_px _mm_load1_ps
+#define _mm_max_px _mm_max_ps
+#define _mm_sub_px _mm_sub_ps
+#define _mm_set1_px _mm_set1_ps
+#define _mm_sqrt_px _mm_sqrt_ps
+#define _mm_div_px _mm_div_ps
+#elif defined(LEGO_USE_DOUBLE)
+        #define __m256x __m256d
+#define __m128x __m128d
+static const unsigned int AVX_STEP_SIZE = 4;
+static const unsigned int SSE_STEP_SIZE = 2;
+static const unsigned int AVX_CUT_LEN_MASK = 3U;
+static const unsigned int SSE_CUT_LEN_MASK = 1U;
+#define _mm256_setzero_px _mm256_setzero_pd
+#define _mm256_mul_px _mm256_mul_pd
+#define _mm256_add_px _mm256_add_pd
+#define _mm256_load_px _mm256_loadu_pd
+#define _mm256_hadd_px _mm256_hadd_pd
+#define _mm256_permute2f128_px _mm256_permute2f128_pd
+#define _mm256_store_px _mm256_storeu_pd
+#define _mm256_broadcast_sx _mm256_broadcast_sd
+#define _mm256_castpx256_px128 _mm256_castpd256_pd128
+#define _mm256_max_px _mm256_max_pd
+#define _mm256_sub_px _mm256_sub_pd
+#define _mm256_set1_px _mm256_set1_pd
+#define _mm256_sqrt_px _mm256_sqrt_pd
+#define _mm256_div_px _mm256_div_pd
+#define _mm_setzero_px _mm_setzero_pd
+#define _mm_add_px _mm_add_pd
+#define _mm_mul_px _mm_mul_pd
+#define _mm_load_px _mm_loadu_pd
+#define _mm_hadd_px _mm_hadd_pd
+#define _mm_store_sx _mm_store_sd
+#define _mm_store_px _mm_storeu_pd
+#define _mm_load1_px _mm_load1_pd
+#define _mm_max_px _mm_max_pd
+#define _mm_sub_px _mm_sub_pd
+#define _mm_set1_px _mm_set1_pd
+#define _mm_sqrt_px _mm_sqrt_pd
+#define _mm_div_px _mm_div_pd
+#endif
+#if defined(LEGO_USE_FLOAT)
+#define X_MIN FLT_MIN
+#define X_MAX FLT_MAX
+#elif defined(LEGO_USE_DOUBLE)
+        #define X_MIN DBL_MIN
+#define X_MAX DBL_MAX
+#endif
+        template <typename T>
+        inline void sse_eltadd(const T* x, const T* y, T* z, size_t len) {
+            unsigned int jjj, lll;
+            jjj = lll = 0;
+#if defined(LEGO_AVX)
+            lll = len & ~AVX_CUT_LEN_MASK;
+  for (jjj = 0; jjj < lll; jjj += AVX_STEP_SIZE) {
+    _mm256_store_px(z + jjj, _mm256_add_px(_mm256_load_px(x + jjj),
+                                           _mm256_load_px(y + jjj)));
+  }
+#elif defined(LEGO_SSE)
+            lll = len & ~SSE_CUT_LEN_MASK;
+            for (jjj = 0; jjj < lll; jjj += SSE_STEP_SIZE) {
+                _mm_store_px(z + jjj,
+                             _mm_add_px(_mm_load_px(x + jjj), _mm_load_px(y + jjj)));
+            }
+#endif
+            for (; jjj < len; jjj++) {
+                z[jjj] = x[jjj] + y[jjj];
+            }
+        }
+        template <typename T>
+        inline void sse_axpy(const T* x, T* y, size_t len, const T alpha) {
+            unsigned int jjj, lll;
+            jjj = lll = 0;
+#if defined(LEGO_AVX)
+            lll = len & ~AVX_CUT_LEN_MASK;
+  __m256x mm_alpha = _mm256_broadcast_sx(&alpha);
+  for (jjj = 0; jjj < lll; jjj += AVX_STEP_SIZE) {
+    _mm256_store_px(
+        y + jjj,
+        _mm256_add_px(_mm256_load_px(y + jjj),
+                      _mm256_mul_px(mm_alpha, _mm256_load_px(x + jjj))));
+  }
+#elif defined(LEGO_SSE)
+            lll = len & ~SSE_CUT_LEN_MASK;
+            __m128x mm_alpha = _mm_load1_px(&alpha);
+            for (jjj = 0; jjj < lll; jjj += SSE_STEP_SIZE) {
+                _mm_store_px(y + jjj,
+                             _mm_add_px(_mm_load_px(y + jjj),
+                                        _mm_mul_px(mm_alpha, _mm_load_px(x + jjj))));
+            }
+#endif
+            for (; jjj < len; jjj++) {
+                y[jjj] += alpha * x[jjj];
+            }
+        }
+        template <typename T>
+        inline void sse_axpy_noadd(const T* x, T* y, size_t len, const T alpha) {
+            unsigned int jjj, lll;
+            jjj = lll = 0;
+#if defined(LEGO_AVX)
+            lll = len & ~AVX_CUT_LEN_MASK;
+  __m256x mm_alpha = _mm256_broadcast_sx(&alpha);
+  for (jjj = 0; jjj < lll; jjj += AVX_STEP_SIZE) {
+    _mm256_store_px(y + jjj, _mm256_mul_px(mm_alpha, _mm256_load_px(x + jjj)));
+  }
+#elif defined(LEGO_SSE)
+            lll = len & ~SSE_CUT_LEN_MASK;
+            __m128x mm_alpha = _mm_load1_px(&alpha);
+            for (jjj = 0; jjj < lll; jjj += SSE_STEP_SIZE) {
+                _mm_store_px(y + jjj, _mm_mul_px(mm_alpha, _mm_load_px(x + jjj)));
+            }
+#endif
+            for (; jjj < len; jjj++) {
+                y[jjj] = alpha * x[jjj];
+            }
+        }
+        template <typename T>
+        inline void sse_eltmul(const T* x, const T* y, T* z, size_t len) {
+            unsigned int jjj, lll;
+            jjj = lll = 0;
+#if defined(LEGO_AVX)
+            lll = len & ~AVX_CUT_LEN_MASK;
+  for (jjj = 0; jjj < lll; jjj += AVX_STEP_SIZE) {
+    _mm256_store_px(z + jjj, _mm256_mul_px(_mm256_load_px(x + jjj),
+                                           _mm256_load_px(y + jjj)));
+  }
+#elif defined(LEGO_SSE)
+            lll = len & ~SSE_CUT_LEN_MASK;
+            for (jjj = 0; jjj < lll; jjj += SSE_STEP_SIZE) {
+                _mm_store_px(z + jjj,
+                             _mm_mul_px(_mm_load_px(x + jjj), _mm_load_px(y + jjj)));
+            }
+#endif
+            for (; jjj < len; jjj++) {
+                z[jjj] = x[jjj] * y[jjj];
+            }
+        }
+        template <typename T>
+        inline void sse_add_scalar(const T* x, T* y, size_t len, const T alpha) {
+            unsigned int jjj, lll;
+            jjj = lll = 0;
+#if defined(LEGO_AVX)
+            lll = len & ~AVX_CUT_LEN_MASK;
+  __m256x mm_alpha = _mm256_broadcast_sx(&alpha);
+  for (jjj = 0; jjj < lll; jjj += AVX_STEP_SIZE) {
+    _mm256_store_px(y + jjj, _mm256_add_px(mm_alpha, _mm256_load_px(x + jjj)));
+  }
+#elif defined(LEGO_SSE)
+            lll = len & ~SSE_CUT_LEN_MASK;
+            __m128x mm_alpha = _mm_load1_px(&alpha);
+            for (jjj = 0; jjj < lll; jjj += SSE_STEP_SIZE) {
+                _mm_store_px(y + jjj, _mm_add_px(mm_alpha, _mm_load_px(x + jjj)));
+            }
+#endif
+            for (; jjj < len; jjj++) {
+                y[jjj] = alpha + x[jjj];
+            }
+        }
+        template <typename T>
+        inline void sse_sum(const T* x, T& y, size_t len) {
+            unsigned int jjj, lll;
+            jjj = lll = 0;
+            y = 0.;
+#if defined(LEGO_AVX)
+            lll = len & ~AVX_CUT_LEN_MASK;
+  __m256x mm_result = _mm256_setzero_px();
+  for (jjj = 0; jjj < lll; jjj += AVX_STEP_SIZE) {
+    mm_result = _mm256_add_px(mm_result, _mm256_load_px(x + jjj));
+  }
+#if defined(LEGO_USE_FLOAT)
+  __m256x hsum = _mm256_hadd_px(mm_result, mm_result);
+#elif defined(LEGO_USE_DOUBLE)
+  __m256x hsum = mm_result;
+#endif
+  hsum = _mm256_add_px(hsum, _mm256_permute2f128_px(hsum, hsum, 0x1));
+  _mm_store_sx(&y, _mm_hadd_px(_mm256_castpx256_px128(hsum),
+                               _mm256_castpx256_px128(hsum)));
+#elif defined(LEGO_SSE)
+            lll = len & ~SSE_CUT_LEN_MASK;
+            __m128x mm_result = _mm_setzero_px();
+            for (jjj = 0; jjj < lll; jjj += SSE_STEP_SIZE) {
+                mm_result = _mm_add_px(mm_result, _mm_load_px(x + jjj));
+            }
+            __m128x mm_tmp = _mm_hadd_px(mm_result, mm_result);
+#if defined(LEGO_USE_FLOAT)
+            _mm_store_sx(&y, _mm_hadd_px(mm_tmp, mm_tmp));
+#elif defined(LEGO_USE_DOUBLE)
+            _mm_store_sx(&y, mm_tmp);
+#endif
+#endif
+            for (; jjj < len; jjj++) {
+                y += x[jjj];
+            }
+        }
+        template <typename T>
+        inline void sse_scale(const T* x, T* y, size_t len, const T alpha) {
+            unsigned int jjj, lll;
+            jjj = lll = 0;
+#if defined(LEGO_AVX)
+            lll = len & ~AVX_CUT_LEN_MASK;
+  __m256x mm_alpha = _mm256_broadcast_sx(&alpha);
+  for (jjj = 0; jjj < lll; jjj += AVX_STEP_SIZE) {
+    _mm256_store_px(y + jjj, _mm256_mul_px(mm_alpha, _mm256_load_px(x + jjj)));
+  }
+#elif defined(LEGO_SSE)
+            lll = len & ~SSE_CUT_LEN_MASK;
+            __m128x mm_alpha = _mm_load1_px(&alpha);
+            for (jjj = 0; jjj < lll; jjj += SSE_STEP_SIZE) {
+                _mm_store_px(y + jjj, _mm_mul_px(mm_alpha, _mm_load_px(x + jjj)));
+            }
+#endif
+            for (; jjj < len; jjj++) {
+                y[jjj] = alpha * x[jjj];
+            }
+        }
+        template <typename T>
+        inline void sse_ip(const T* vec1, const T* vec2, size_t len, T& result) {
+            unsigned int jjj, lll;
+            jjj = lll = 0;
+            result = 0.;
+#if defined(LEGO_AVX)
+            lll = len & ~AVX_CUT_LEN_MASK;
+  __m256x mm_result = _mm256_setzero_px();
+  for (jjj = 0; jjj < lll; jjj += AVX_STEP_SIZE) {
+    mm_result = _mm256_add_px(
+        mm_result,
+        _mm256_mul_px(_mm256_load_px(vec1 + jjj), _mm256_load_px(vec2 + jjj)));
+  }
+  //    result = mm_result[0]+mm_result[1]+mm_result[2]+mm_result[3]+
+  //      mm_result[4]+mm_result[5]+mm_result[6]+mm_result[7];
+#if defined(LEGO_USE_FLOAT)
+  __m256x hsum = _mm256_hadd_px(mm_result, mm_result);
+#elif defined(LEGO_USE_DOUBLE)
+  __m256x hsum = mm_result;
+#endif
+  hsum = _mm256_add_px(hsum, _mm256_permute2f128_px(hsum, hsum, 0x1));
+  _mm_store_sx(&result, _mm_hadd_px(_mm256_castpx256_px128(hsum),
+                                    _mm256_castpx256_px128(hsum)));
+#elif defined(LEGO_SSE)
+            lll = len & ~SSE_CUT_LEN_MASK;
+            __m128x mm_result = _mm_setzero_px();
+            for (jjj = 0; jjj < lll; jjj += SSE_STEP_SIZE) {
+                mm_result = _mm_add_px(mm_result, _mm_mul_px(_mm_load_px(vec1 + jjj),
+                                                             _mm_load_px(vec2 + jjj)));
+            }
+            __m128x mm_tmp = _mm_hadd_px(mm_result, mm_result);
+#if defined(LEGO_USE_FLOAT)
+            _mm_store_sx(&result, _mm_hadd_px(mm_tmp, mm_tmp));
+#elif defined(LEGO_USE_DOUBLE)
+            _mm_store_sx(&result, mm_tmp);
+#endif
+#endif
+            for (; jjj < len; jjj++) {
+                result += vec1[jjj] * vec2[jjj];
+            }
+        }
+    }  // namespace operators
+}  // namespace paddle
diff --git a/paddle/fluid/operators/search_fc_op.cc b/paddle/fluid/operators/search_fc_op.cc
new file mode 100644
index 0000000000..42340959a0
--- /dev/null
+++ b/paddle/fluid/operators/search_fc_op.cc
@@ -0,0 +1,163 @@
+/* Copyright (c) 2018 PaddlePaddle Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License. */
+#include <cmath>
+#include "paddle/fluid/framework/op_registry.h"
+#include "paddle/fluid/operators/search_compute_2.h"
+namespace paddle {
+namespace operators {
+    using Tensor = framework::Tensor;
+    using LoDTensor = framework::LoDTensor;
+    using LoD = framework::LoD;
+
+    class SearchFCOpMaker : public framework::OpProtoAndCheckerMaker {
+    public:
+        void Make() override {
+            AddInput("X",
+                     "X (Tensor, default Tensor<float>) Input variable which "
+                     "should contain lod information.");
+            AddInput("W", "W (Tensor)");
+            AddInput("b", "b (Tensor)");
+            AddAttr<int>("out_size", "out_size: the output size")
+                    .SetDefault(0)
+                    .EqualGreaterThan(1);
+            AddOutput("Out", "Out (Tensor, default Tensor<float>) Output variable");
+            AddComment(R"DOC(
+  SearchFC
+
+  NOTE: only support 'float32' data type now.
+)DOC");
+        }
+    };
+    class SearchFCOP : public framework::OperatorWithKernel {
+    public:
+        using framework::OperatorWithKernel::OperatorWithKernel;
+        void InferShape(framework::InferShapeContext* ctx) const override {
+            PADDLE_ENFORCE(ctx->HasInput("X"), "X(Input) should not be null.");
+            PADDLE_ENFORCE(ctx->HasInput("W"), "W(Input) should not be null.");
+            PADDLE_ENFORCE(ctx->HasInput("b"), "b(Input) should not be null.");
+            PADDLE_ENFORCE(ctx->HasOutput("Out"), "Out(Output) should not be null.");
+            auto x_dims = ctx->GetInputDim("X");
+            PADDLE_ENFORCE_EQ(x_dims.size(), 2, "The rank of X(Input) should be 2.");
+            auto w_dims = ctx->GetInputDim("W");
+            PADDLE_ENFORCE_EQ(w_dims.size(), 2, "W should be 2-D tensor");
+            auto b_dims = ctx->GetInputDim("b");
+            PADDLE_ENFORCE_EQ(b_dims.size(), 1, "b should be 1-D tensor");
+            int out_size = ctx->Attrs().Get<int>("out_size");
+            ctx->SetOutputDim("Out", framework::make_ddim({-1, out_size}));
+            if (ctx->IsRuntime()) {
+                PADDLE_ENFORCE_EQ(w_dims[1], x_dims[1], "wrong shape: w_dims[1] != x_dims[1]");
+            }
+            else {
+                // compile time
+            }
+        }
+    };
+    template <typename DeviceContext, typename T>
+    class CPUSearchFCOPKernel : public framework::OpKernel<T> {
+    public:
+        void Compute(const framework::ExecutionContext& ctx) const override {
+            auto* bottom = ctx.Input<Tensor>("X");
+            auto* w = ctx.Input<Tensor>("W");
+            auto* b = ctx.Input<Tensor>("b");
+            auto* top = ctx.Output<Tensor>("Out");
+            int out_size = ctx.Attr<int>("out_size");  // 100
+            int batch = bottom->dims()[0];
+            int _out = w->dims()[0];  // 100
+            int _in = w->dims()[1];   // 228
+            top->Resize(framework::make_ddim({bottom->dims()[0], out_size}));
+            const auto* bottom_data = bottom->data<T>();
+            auto* top_data = top->mutable_data<T>(ctx.GetPlace());
+            const auto* weights = w->data<T>();
+            auto blas = math::GetBlas<platform::CPUDeviceContext, T>(ctx);
+            call_gemm(blas, CblasNoTrans, CblasTrans, batch, _out, _in, 1.0f,
+                      bottom_data, weights, 0.0f, top_data);
+            if (true) {
+                const auto* bias_data = b->data<T>();
+                for (int i = 0; i < batch; ++i) {
+                    // add bias here
+                    sse_eltadd(top_data + i * _out, bias_data, top_data + i * _out, _out);
+                }
+            }
+        }
+    };
+    class SearchFCOpGrad : public framework::OperatorWithKernel {
+    public:
+        using framework::OperatorWithKernel::OperatorWithKernel;
+        void InferShape(framework::InferShapeContext* ctx) const override {
+            PADDLE_ENFORCE(ctx->HasInput("X"), "Input(X) should not be null.");
+            PADDLE_ENFORCE(ctx->HasInput("W"), "Input(W) should not be null.");
+            PADDLE_ENFORCE(ctx->HasInput("b"), "Input(b) should not be null.");
+            PADDLE_ENFORCE(ctx->HasInput(framework::GradVarName("Out")),
+                           "Input(Out@GRAD) of SequencePadGradOp should not be null.");
+            if (ctx->HasOutput(framework::GradVarName("X"))) {
+                ctx->SetOutputDim(framework::GradVarName("X"), ctx->GetInputDim("X"));
+            }
+            if (ctx->HasOutput(framework::GradVarName("W"))) {
+                ctx->SetOutputDim(framework::GradVarName("W"), ctx->GetInputDim("W"));
+            }
+            if (ctx->HasOutput(framework::GradVarName("b"))) {
+                ctx->SetOutputDim(framework::GradVarName("b"), ctx->GetInputDim("b"));
+            }
+        }
+    };
+    template <typename DeviceContext, typename T>
+    class CPUSearchFCOPGradKernel : public framework::OpKernel<T> {
+    public:
+        void Compute(const framework::ExecutionContext& ctx) const override {
+            auto* bottom = ctx.Input<Tensor>("X");
+            auto* w = ctx.Input<Tensor>("W");
+            int _out = w->dims()[0];  // 100
+            int _in = w->dims()[1];   // 228
+            auto* d_out = ctx.Input<Tensor>(framework::GradVarName("Out"));
+            auto* d_x = ctx.Output<Tensor>(framework::GradVarName("X"));
+            auto* d_w = ctx.Output<Tensor>(framework::GradVarName("W"));
+            int batch = bottom->dims()[0];
+            const auto* top_diff = d_out->data<T>();
+            const auto* bottom_data = bottom->data<T>();
+            auto* bottom_diff = d_x->mutable_data<T>(ctx.GetPlace());
+            const auto* weights = w->data<T>();
+            auto* weights_diff = d_w->mutable_data<T>(ctx.GetPlace());
+            auto blas = math::GetBlas<platform::CPUDeviceContext, T>(ctx);
+            call_gemm(blas, CblasTrans, CblasNoTrans, _out, _in, batch, (T)1.0,
+                      top_diff, bottom_data, (T)0.0, weights_diff);
+            call_gemm(blas, CblasNoTrans, CblasNoTrans, batch, _in, _out, (T)1.0, top_diff,
+                      weights, (T)0.0, bottom_diff);
+            if (true) {
+                auto* d_b = ctx.Output<Tensor>(framework::GradVarName("b"));
+                auto* bias_diff = d_b->mutable_data<T>(ctx.GetPlace());
+                memset(bias_diff, 0.0, _out * sizeof(T));
+                for (int i = 0; i < batch; ++i) {
+                    sse_eltadd(bias_diff, top_diff + i * _out, bias_diff, _out);
+                }
+
+            }
+        }
+    };
+}  // namespace operators
+}  // namespace paddle
+namespace ops = paddle::operators;
+namespace plt = paddle::platform;
+namespace frm = paddle::framework;
+REGISTER_OPERATOR(search_fc, ops::SearchFCOP, ops::SearchFCOpMaker,
+                  frm::DefaultGradOpMaker<paddle::framework::OpDesc, true>,
+                  frm::DefaultGradOpMaker<paddle::imperative::OpBase, true>);
+REGISTER_OPERATOR(search_fc_grad, ops::SearchFCOpGrad);
+REGISTER_OP_CPU_KERNEL(search_fc,
+                       ops::CPUSearchFCOPKernel<plt::CPUDeviceContext, float>
+//     ops::CPUSearchFCOPKernel<plt::CPUDeviceContext,
+//                                       double>
+);
+REGISTER_OP_CPU_KERNEL(
+        search_fc_grad, ops::CPUSearchFCOPGradKernel<plt::CPUDeviceContext, float>
+//     ops::CPUSearchFCOPGradKernel<plt::CPUDeviceContext,
+//                                           double>
+);
+
diff --git a/paddle/fluid/operators/search_grnn_op.cc b/paddle/fluid/operators/search_grnn_op.cc
new file mode 100644
index 0000000000..c01a32e56c
--- /dev/null
+++ b/paddle/fluid/operators/search_grnn_op.cc
@@ -0,0 +1,541 @@
+/* Copyright (c) 2018 PaddlePaddle Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License. */
+#include "paddle/fluid/operators/search_grnn_op.h"
+#include "paddle/fluid/operators/search_compute_2.h"
+
+//#include "debug.h"
+namespace paddle {
+    namespace operators {
+        using Tensor = framework::Tensor;
+        using LoDTensor = framework::LoDTensor;
+        using LoD = framework::LoD;
+#define SIGMOID(z) (sigmoid(z))
+#define SIGMOID_D(a) ((a) * (1 - (a)))
+#define TANHD(a) (1 - (a) * (a))
+
+        template <typename T>
+        T sigmoid(T z) {
+            return 1 / (1 + std::exp(-z));
+        }
+
+        class SearchGrnnOpMaker : public framework::OpProtoAndCheckerMaker {
+        public:
+            void Make() override {
+                AddInput("X",
+                         "X (LoDTensor, default LoDTensor<float>) Input variable which "
+                         "should contain lod information.");
+                AddInput("Wi", "Wi (Tensor)");
+                AddInput("Wh", "Wh (Tensor)");
+                AddAttr<int>("num_input", "num_input: the embedding size").SetDefault(0);
+                AddAttr<int>("num_hidden", "num_hidden: the hidden size").SetDefault(0);
+                AddOutput("Out",
+                          "Out (LoDTensor, default LoDTensor<float>) Output variable");
+                AddOutput("tmp_buffer",
+                          "tmp_buffer (LoDTensor, default LoDTensor<float>) tmp variable");
+                AddOutput("idx_sorted_by_width",
+                          "idx_sorted_by_width (Tensor, Tensor<int>) tmp variable");
+                AddOutput(
+                        "layout_input",
+                        "layout_input (LoDTensor, default LoDTensor<float>) tmp variable");
+                AddComment(R"DOC(
+  SearchGrnn
+
+  NOTE: only support 'float32' data type now.
+)DOC");
+            }
+        };
+        class SearchGrnnOP : public framework::OperatorWithKernel {
+        public:
+            using framework::OperatorWithKernel::OperatorWithKernel;
+            void InferShape(framework::InferShapeContext* ctx) const override {
+                PADDLE_ENFORCE(ctx->HasInput("X"), "X(Input) should not be null.");
+                PADDLE_ENFORCE(ctx->HasInput("Wi"), "Wi(Input) should not be null.");
+                PADDLE_ENFORCE(ctx->HasInput("Wh"), "Wh(Input) should not be null.");
+                PADDLE_ENFORCE(ctx->HasOutput("Out"), "Out(Output) should not be null.");
+                PADDLE_ENFORCE(ctx->HasOutput("tmp_buffer"),
+                               "tmp_buffer(Output) should not be null.");
+                PADDLE_ENFORCE(ctx->HasOutput("idx_sorted_by_width"),
+                               "idx_sorted_by_width(Output) should not be null.");
+                PADDLE_ENFORCE(ctx->HasOutput("layout_input"),
+                               "layout_input(Output) should not be null.");
+                int _cap_h = ctx->Attrs().Get<int>("num_hidden");
+                int _cap_e = ctx->Attrs().Get<int>("num_input");
+                auto x_dims = ctx->GetInputDim("X");
+                PADDLE_ENFORCE_EQ(x_dims.size(), 2,
+                                  "The rank of X(Input) can't be less than 2.");
+                PADDLE_ENFORCE_EQ(x_dims[1], _cap_e, "x_dims[1] should be equal to _cap_e");
+                auto wi_dims = ctx->GetInputDim("Wi");
+                PADDLE_ENFORCE_EQ(wi_dims.size(), 3, "Wi should be 3-D tensor");
+                PADDLE_ENFORCE_EQ(wi_dims[0], 3, "Wi dim[0] should be equal to 3");
+                PADDLE_ENFORCE_EQ(wi_dims[1], _cap_h,
+                                  "wi_dims[1] should be equal to _cap_h");
+                PADDLE_ENFORCE_EQ(wi_dims[2], _cap_e,
+                                  "wi_dims[2] should be equal to _cap_e");
+                auto wh_dims = ctx->GetInputDim("Wh");
+                PADDLE_ENFORCE_EQ(wh_dims.size(), 3, "Wi should be 3-D tensor");
+                PADDLE_ENFORCE_EQ(wh_dims[0], 3, "Wh dim[0] should be equal to 3");
+                PADDLE_ENFORCE_EQ(wh_dims[1], _cap_h,
+                                  "wh_dims[1] should be equal to _cap_h");
+                PADDLE_ENFORCE_EQ(wh_dims[2], _cap_h,
+                                  "wh_dims[2] should be equal to _cap_h");
+                if (ctx->IsRuntime()) {
+                    framework::Variable* x_var =
+                            boost::get<framework::Variable*>(ctx->GetInputVarPtrs("X")[0]);
+                    const auto& x_lod = x_var->Get<LoDTensor>().lod();
+                    PADDLE_ENFORCE(!x_lod.empty(), "The Input(X) must hold lod info.");
+                    PADDLE_ENFORCE_EQ(
+                            x_dims[0], static_cast<int64_t>(x_lod[0].back()),
+                            "The Input(X)'s lod info mismatches the actual tensor shape.");
+                } else {
+                    std::vector<int64_t> out_dims_vec{-1};
+                    out_dims_vec.push_back(_cap_h);
+                    std::vector<int64_t> tmp_buffer_shape{20};
+                    tmp_buffer_shape.push_back(-1);
+                    tmp_buffer_shape.push_back(_cap_h);
+                    ctx->SetOutputDim("Out", framework::make_ddim(out_dims_vec));
+                    ctx->SetOutputDim("tmp_buffer", framework::make_ddim(tmp_buffer_shape));
+                }
+                ctx->ShareLoD("X", /*->*/ "Out");
+            }
+        protected:
+            framework::OpKernelType GetExpectedKernelType(
+                    const framework::ExecutionContext& ctx) const override {
+                return framework::OpKernelType(
+                        OperatorWithKernel::IndicateVarDataType(ctx, "X"),
+                        ctx.device_context());
+            }
+        };
+        template <typename DeviceContext, typename T>
+        class CPUSearchGrnnOPKernel : public framework::OpKernel<T> {
+        public:
+            void prepare_layout(const framework::ExecutionContext& ctx,
+                                const LoDTensor* input_blob) const {
+                auto* _idx_sorted_by_width = ctx.Output<Tensor>("idx_sorted_by_width");
+                auto* _layout_input = ctx.Output<LoDTensor>("layout_input");
+                auto _input = input_blob;
+                // usually total length
+                int dim0 = _input->dims()[0];
+                // if it is id only sequence
+                int dim1 = 1;
+                // if its a embedding like sequence (dim1 would be embedding_size)
+                if (_input->dims().size() > 1) {
+                    dim1 = _input->dims()[1];
+                }
+                int batch = _input->lod()[0].size() - 1;
+                auto& offset = _input->lod()[0];
+                Tensor _width;
+                _width.Resize(framework::make_ddim({batch}));
+                _idx_sorted_by_width->Resize(framework::make_ddim({batch}));
+                int* width_data = _width.mutable_data<int>(ctx.GetPlace());
+                int* idx_sorted_by_width_data =
+                        _idx_sorted_by_width->mutable_data<int>(ctx.GetPlace());
+                // sort sequence by width (descending) and find the largest width in the
+                // batch
+                for (int i = 0; i < batch; i++) {
+                    width_data[i] = offset[i + 1] - offset[i];
+                    idx_sorted_by_width_data[i] = i;
+                }
+                std::sort(idx_sorted_by_width_data, idx_sorted_by_width_data + batch,
+                          [&_width](int a, int b) {
+                              return _width.data<int>()[a] > _width.data<int>()[b];
+                          });
+                int max_width = width_data[idx_sorted_by_width_data[0]];
+                // start of reorganizing the input
+                std::vector<size_t> new_offset;
+                new_offset.resize(max_width + 1);
+                new_offset[0] = 0;
+                int j = batch - 1;
+                int last_width = 0;
+                int sub_row = 0;
+                int sub_col = 0;
+                for (int i = 1; i <= max_width;) {
+                    for (int k = j; k >= 0; --k) {
+                        if (width_data[idx_sorted_by_width_data[k]] > last_width) {
+                            sub_row = width_data[idx_sorted_by_width_data[k]] - last_width;
+                            sub_col = k + 1;
+                            for (int s = 0; s < sub_row; s++) {
+                                new_offset[i] = new_offset[i - 1] + sub_col;
+                                i++;
+                            }
+                            // move on
+                            last_width = width_data[idx_sorted_by_width_data[k]];
+                            j = k - 1;
+                            break;
+                        }
+                    }
+                }
+                // copying to the reorganized buffer
+                if (_input->dims().size() == 1) {
+                    //_layout_input.reshape_batch_sequence({dim0}, new_offset);
+                } else {
+                    //_layout_input.reshape_batch_sequence({dim0, dim1}, new_offset);
+                    framework::LoD new_lod;
+                    new_lod.push_back(new_offset);
+                    _layout_input->set_lod(new_lod);
+                    _layout_input->Resize(framework::make_ddim({dim0, dim1}));
+                }
+                auto* new_emb = _layout_input->mutable_data<T>(ctx.GetPlace());
+                for (int i = 0; i < max_width; i++) {
+                    int w = new_offset[i + 1] - new_offset[i];
+                    auto* emb_start = new_emb + dim1 * new_offset[i];
+                    for (int j = 0; j < w; ++j) {
+                        memcpy(emb_start + dim1 * j,
+                               _input->data<T>() + dim1 * offset[idx_sorted_by_width_data[j]] +
+                               dim1 * i,
+                               dim1 * sizeof(T));
+                    }
+                }
+                // end of reorganizing the input
+            }
+            void copy_back(const framework::ExecutionContext& ctx, T* from, T* to,
+                           int step) const {
+                auto* _input = ctx.Input<LoDTensor>("X");
+                auto* _layout_input = ctx.Output<LoDTensor>("layout_input");
+                auto* _idx_sorted_by_width = ctx.Output<Tensor>("idx_sorted_by_width");
+                const auto& offset = _input->lod()[0];
+                const auto& new_offset = _layout_input->lod()[0];
+                const auto* idx_sorted_by_width_data = _idx_sorted_by_width->data<int>();
+                for (size_t i = 0; i < _layout_input->lod()[0].size() - 1; ++i) {
+                    int w = new_offset[i + 1] - new_offset[i];
+                    for (int j = 0; j < w; j++) {
+                        memcpy(to + step * (offset[idx_sorted_by_width_data[j]] + i),
+                               from + (new_offset[i] + j) * step, step * sizeof(T));
+                    }
+                }
+            }
+            void Compute(const framework::ExecutionContext& ctx) const override {
+                auto* bottom = ctx.Input<LoDTensor>("X");
+                auto* wi = ctx.Input<LoDTensor>("Wi");
+                auto* wh = ctx.Input<Tensor>("Wh");
+                auto* top = ctx.Output<LoDTensor>("Out");
+                auto* _buffer = ctx.Output<LoDTensor>("tmp_buffer");
+                // std::vector<const LoDTensor*> _blobs{wi, wh};
+                int _cap_h = ctx.Attr<int>("num_hidden");
+                int _cap_e = ctx.Attr<int>("num_input");
+                int _cap_l = bottom->dims()[0];
+                int batch = bottom->lod()[0].size() - 1;
+                const auto& offset = bottom->lod()[0];
+                framework::LoD top_lod;
+                top_lod.push_back(offset);
+                top->set_lod(top_lod);
+                std::vector<int64_t> top_dims_vec{_cap_l, _cap_h};
+                auto* top_hidden = top->mutable_data<T>(framework::make_ddim(top_dims_vec),
+                                                        ctx.GetPlace());
+                const auto* dense_e2h = wi->data<T>();
+                const auto* dense_h2h = wh->data<T>();
+                const auto* e2h = dense_e2h;
+                const auto* e2hr = dense_e2h + 1 * _cap_e * _cap_h;
+                const auto* e2hz = dense_e2h + 2 * _cap_e * _cap_h;
+                const auto* h2h = dense_h2h;
+                const auto* h2hr = dense_h2h + 1 * _cap_h * _cap_h;
+                const auto* h2hz = dense_h2h + 2 * _cap_h * _cap_h;
+                prepare_layout(ctx, bottom);
+                auto* _layout_input = ctx.Output<LoDTensor>("layout_input");
+                auto* new_emb = _layout_input->mutable_data<T>(ctx.GetPlace());
+                const auto& new_offset = _layout_input->lod()[0];
+                int max_width = _layout_input->lod()[0].size() - 1;
+                // this buffer is used for book keeping info which will be used in bp
+                // buffer also needed in bp, so make it larger
+                _buffer->Resize(framework::make_ddim({20, _cap_l, _cap_h}));
+                auto* buffer_data = _buffer->mutable_data<T>(ctx.GetPlace());
+                auto* w_x_e = buffer_data + 0 * _cap_l * _cap_h;
+                auto* wr_x_e = buffer_data + 1 * _cap_l * _cap_h;
+                auto* wz_x_e = buffer_data + 2 * _cap_l * _cap_h;
+                auto* u_x_h = buffer_data + 3 * _cap_l * _cap_h;
+                auto* ur_x_h = buffer_data + 4 * _cap_l * _cap_h;
+                auto* uz_x_h = buffer_data + 5 * _cap_l * _cap_h;
+                auto* r = buffer_data + 6 * _cap_l * _cap_h;
+                auto* z = buffer_data + 7 * _cap_l * _cap_h;
+                auto* tilde = buffer_data + 8 * _cap_l * _cap_h;
+                // the internal hidden
+                auto* hidden = buffer_data + 19 * _cap_l * _cap_h;
+                // precompute embedding to hidden
+                auto blas = math::GetBlas<platform::CPUDeviceContext, T>(ctx);
+                call_gemm(blas, CblasNoTrans, CblasTrans, _cap_l, _cap_h, _cap_e, 1.0f,
+                          new_emb, e2h, 0.0f, w_x_e);
+                call_gemm(blas, CblasNoTrans, CblasTrans, _cap_l, _cap_h, _cap_e, 1.0f,
+                          new_emb, e2hr, 0.0f, wr_x_e);
+                call_gemm(blas, CblasNoTrans, CblasTrans, _cap_l, _cap_h, _cap_e, 1.0f,
+                          new_emb, e2hz, 0.0f, wz_x_e);
+                // precompute hidden0
+                for (int i = 0; i < batch * _cap_h; i++) {
+                    tilde[i] = std::tanh(w_x_e[i]);
+                    z[i] = sigmoid<T>(wz_x_e[i]);
+                    hidden[i] = (1. - z[i]) * tilde[i];
+                }
+                // recurrence
+                for (int i = 1; i < max_width; i++) {
+                    int w_tm1 = new_offset[i] - new_offset[i - 1];
+                    int w = new_offset[i + 1] - new_offset[i];
+                    // precompute hidden i-1 to hidden i
+                    auto* htm1 = hidden + new_offset[i - 1] * _cap_h;
+                    call_gemm(blas, CblasNoTrans, CblasTrans, w, _cap_h, _cap_h, 1.0f, htm1,
+                              h2h, 0.0f, u_x_h + new_offset[i] * _cap_h);
+                    call_gemm(blas, CblasNoTrans, CblasTrans, w, _cap_h, _cap_h, 1.0f, htm1,
+                              h2hr, 0.0f, ur_x_h + new_offset[i] * _cap_h);
+                    call_gemm(blas, CblasNoTrans, CblasTrans, w, _cap_h, _cap_h, 1.0f, htm1,
+                              h2hz, 0.0f, uz_x_h + new_offset[i] * _cap_h);
+                    // compute the gate and hidden
+                    for (size_t j = new_offset[i] * _cap_h; j < (new_offset[i] + w) * _cap_h;
+                         j++) {
+                        r[j] = sigmoid(wr_x_e[j] + ur_x_h[j]);
+                        z[j] = sigmoid(wz_x_e[j] + uz_x_h[j]);
+                        tilde[j] = std::tanh(w_x_e[j] + r[j] * u_x_h[j]);
+                        hidden[j] = z[j] * hidden[j - _cap_h * w_tm1] + (1.0 - z[j]) * tilde[j];
+                    }
+                }
+                // copy back to top
+                copy_back(ctx, hidden, top_hidden, _cap_h);
+            }
+        };
+        class SearchGrnnOpGrad : public framework::OperatorWithKernel {
+        public:
+            using framework::OperatorWithKernel::OperatorWithKernel;
+            void InferShape(framework::InferShapeContext* ctx) const override {
+                PADDLE_ENFORCE(ctx->HasInput("X"), "Input(X) should not be null.");
+                PADDLE_ENFORCE(ctx->HasInput("Wi"), "Input(Wi) should not be null.");
+                PADDLE_ENFORCE(ctx->HasInput("Wh"), "Input(Wh) should not be null.");
+                PADDLE_ENFORCE(ctx->HasInput(framework::GradVarName("Out")),
+                               "Input(Out@GRAD) of SequencePadGradOp should not be null.");
+                if (ctx->HasOutput(framework::GradVarName("X"))) {
+                    ctx->SetOutputDim(framework::GradVarName("X"), ctx->GetInputDim("X"));
+                    ctx->ShareLoD("X", /*->*/ framework::GradVarName("X"));
+                }
+                if (ctx->HasOutput(framework::GradVarName("Wi"))) {
+                    ctx->SetOutputDim(framework::GradVarName("Wi"), ctx->GetInputDim("Wi"));
+                }
+                if (ctx->HasOutput(framework::GradVarName("Wh"))) {
+                    ctx->SetOutputDim(framework::GradVarName("Wh"), ctx->GetInputDim("Wh"));
+                }
+            }
+        protected:
+            framework::OpKernelType GetExpectedKernelType(
+                    const framework::ExecutionContext& ctx) const override {
+                return framework::OpKernelType(
+                        OperatorWithKernel::IndicateVarDataType(ctx, "X"),
+                        ctx.device_context());
+            }
+        };
+        template <typename DeviceContext, typename T>
+        class CPUSearchGrnnOPGradKernel : public framework::OpKernel<T> {
+        public:
+            void do_same_layout(const framework::ExecutionContext& ctx, const T* from,
+                                T* to, int step) const {
+                auto* _input = ctx.Input<LoDTensor>("X");
+                auto* _layout_input = ctx.Input<LoDTensor>("layout_input");
+                auto& offset = _input->lod()[0];
+                const auto& new_offset = _layout_input->lod()[0];
+                auto* _idx_sorted_by_width = ctx.Input<Tensor>("idx_sorted_by_width");
+                const int* idx_sorted_by_width_data = _idx_sorted_by_width->data<int>();
+                for (int i = 0; i < _layout_input->lod()[0].size() - 1; i++) {
+                    int w = new_offset[i + 1] - new_offset[i];
+                    for (int j = 0; j < w; j++) {
+                        memcpy(to + (new_offset[i] + j) * step,
+                               from + step * (offset[idx_sorted_by_width_data[j]] + i),
+                               step * sizeof(T));
+                    }
+                }
+            }
+            void copy_back(const framework::ExecutionContext& ctx, T* from, T* to,
+                           int step) const {
+                auto* _input = ctx.Input<LoDTensor>("X");
+                auto* _layout_input = ctx.Input<LoDTensor>("layout_input");
+                auto* _idx_sorted_by_width = ctx.Input<Tensor>("idx_sorted_by_width");
+                const auto& offset = _input->lod()[0];
+                const auto& new_offset = _layout_input->lod()[0];
+                const auto* idx_sorted_by_width_data = _idx_sorted_by_width->data<int>();
+                for (size_t i = 0; i < _layout_input->lod()[0].size() - 1; ++i) {
+                    int w = new_offset[i + 1] - new_offset[i];
+                    for (int j = 0; j < w; j++) {
+                        memcpy(to + step * (offset[idx_sorted_by_width_data[j]] + i),
+                               from + (new_offset[i] + j) * step, step * sizeof(T));
+                    }
+                }
+            }
+            void Compute(const framework::ExecutionContext& ctx) const override {
+                auto* bottom = ctx.Input<LoDTensor>("X");
+                auto* wi = ctx.Input<LoDTensor>("Wi");
+                auto* wh = ctx.Input<Tensor>("Wh");
+                auto* _buffer = ctx.Input<LoDTensor>("tmp_buffer");
+                auto* _layout_input = ctx.Input<LoDTensor>("layout_input");
+                // std::vector<const LoDTensor*> _blobs{wi, wh};
+                int _cap_h = ctx.Attr<int>("num_hidden");
+                int _cap_e = ctx.Attr<int>("num_input");
+                int _cap_l = bottom->dims()[0];
+                auto* d_bottom = ctx.Output<LoDTensor>(framework::GradVarName("X"));
+                auto* d_top = ctx.Input<LoDTensor>(framework::GradVarName("Out"));
+                auto* d_wi = ctx.Output<LoDTensor>(framework::GradVarName("Wi"));
+                auto* d_wh = ctx.Output<LoDTensor>(framework::GradVarName("Wh"));
+                int batch = bottom->lod()[0].size() - 1;
+                const auto& new_offset = _layout_input->lod()[0];
+                int max_width = _layout_input->lod()[0].size() - 1;
+                // the original top and bottom pointers
+                auto* top_diff = d_top->data<T>();
+                auto* ediff = d_bottom->mutable_data<T>(ctx.GetPlace());
+                const auto* dense_e2h = wi->data<T>();
+                const auto* dense_h2h = wh->data<T>();
+                auto* dense_e2h_diff = d_wi->mutable_data<T>(ctx.GetPlace());
+                auto* dense_h2h_diff = d_wh->mutable_data<T>(ctx.GetPlace());
+                // init parameter's diff
+                memset(dense_e2h_diff, 0, 3 * _cap_e * _cap_h * sizeof(T));
+                memset(dense_h2h_diff, 0, 3 * _cap_h * _cap_h * sizeof(T));
+                const auto* e2h = dense_e2h;
+                const auto* e2hr = dense_e2h + 1 * _cap_e * _cap_h;
+                const auto* e2hz = dense_e2h + 2 * _cap_e * _cap_h;
+                const auto* h2h = dense_h2h;
+                const auto* h2hr = dense_h2h + 1 * _cap_h * _cap_h;
+                const auto* h2hz = dense_h2h + 2 * _cap_h * _cap_h;
+                auto* e2h_diff = dense_e2h_diff;
+                auto* e2hr_diff = dense_e2h_diff + 1 * _cap_e * _cap_h;
+                auto* e2hz_diff = dense_e2h_diff + 2 * _cap_e * _cap_h;
+                auto* h2h_diff = dense_h2h_diff;
+                auto* h2hr_diff = dense_h2h_diff + 1 * _cap_h * _cap_h;
+                auto* h2hz_diff = dense_h2h_diff + 2 * _cap_h * _cap_h;
+                auto u_x_h = _buffer->data<T>() + 3 * _cap_l * _cap_h;
+                Tensor buffer_diff;
+                buffer_diff.Resize(framework::make_ddim({20, _cap_l, _cap_h}));
+                auto* buffer_diff_data = buffer_diff.mutable_data<T>(ctx.GetPlace());
+                auto e2hdiff = buffer_diff_data + 0 * _cap_l * _cap_h;
+                auto e2hrdiff = buffer_diff_data + 1 * _cap_l * _cap_h;
+                auto e2hzdiff = buffer_diff_data + 2 * _cap_l * _cap_h;
+                auto h2hdiff = buffer_diff_data + 3 * _cap_l * _cap_h;
+                auto h2hrdiff = buffer_diff_data + 4 * _cap_l * _cap_h;
+                auto h2hzdiff = buffer_diff_data + 5 * _cap_l * _cap_h;
+                auto* buffer_data = _buffer->data<T>();
+                auto r = buffer_data + 6 * _cap_l * _cap_h;
+                auto z = buffer_data + 7 * _cap_l * _cap_h;
+                auto tilde = buffer_data + 8 * _cap_l * _cap_h;
+                auto d_r = buffer_diff_data + 9 * _cap_l * _cap_h;
+                auto d_z = buffer_diff_data + 10 * _cap_l * _cap_h;
+                auto d_tilde = buffer_diff_data + 11 * _cap_l * _cap_h;
+                auto tmp_buffer = buffer_diff_data + 12 * _cap_l * _cap_h;
+                auto hidden = buffer_data + 19 * _cap_l * _cap_h;
+                auto hidden_diff = buffer_diff_data + 19 * _cap_l * _cap_h;
+                auto embedding = _layout_input->data<T>();
+                Tensor _layout_input_grad;
+                _layout_input_grad.Resize(_layout_input->dims());
+                auto embedding_diff = _layout_input_grad.mutable_data<T>(ctx.GetPlace());
+                // copy top_hiddden diff back to the reorganized hidden, so we can use
+                // segemm to back-prop the sequence
+                do_same_layout(ctx, top_diff, hidden_diff, _cap_h);
+                // precompute nonlinear diff
+                for (int k = 0; k < new_offset[1] * _cap_h; k++) {
+                    d_z[k] = SIGMOID_D(z[k]);
+                    d_tilde[k] = TANHD(tilde[k]);
+                }
+                for (int k = new_offset[1] * _cap_h; k < new_offset[max_width] * _cap_h;
+                     k++) {
+                    d_r[k] = SIGMOID_D(r[k]);
+                    d_z[k] = SIGMOID_D(z[k]);
+                    d_tilde[k] = TANHD(tilde[k]);
+                }
+                auto blas = math::GetBlas<platform::CPUDeviceContext, T>(ctx);
+                // back prop
+                for (int i = max_width - 1; i > 0; i--) {
+                    int w_tm1 = new_offset[i] - new_offset[i - 1];
+                    int w = new_offset[i + 1] - new_offset[i];
+                    for (int j = new_offset[i]; j < (new_offset[i] + w); j++) {
+                        for (int k = 0; k < _cap_h; k++) {
+                            int ht = j * _cap_h + k;
+                            int htm1 = ht - _cap_h * w_tm1;
+                            T common = (1.0 - z[ht]) * d_tilde[ht] * hidden_diff[ht];
+                            h2hdiff[htm1] = common * r[ht];
+                            h2hrdiff[htm1] = common * u_x_h[ht] * d_r[ht];
+                            h2hzdiff[htm1] =
+                                    (hidden[htm1] - tilde[ht]) * d_z[ht] * hidden_diff[ht];
+                            e2hdiff[ht] = common;
+                            e2hrdiff[ht] = h2hrdiff[htm1];
+                            e2hzdiff[ht] = h2hzdiff[htm1];
+                        }
+                    }
+                    auto* hidden_htm1 = hidden + new_offset[i - 1] * _cap_h;
+                    auto* h2hdiff_htm1 = h2hdiff + new_offset[i - 1] * _cap_h;
+                    auto* h2hrdiff_htm1 = h2hrdiff + new_offset[i - 1] * _cap_h;
+                    auto* h2hzdiff_htm1 = h2hzdiff + new_offset[i - 1] * _cap_h;
+                    call_gemm(blas, CblasTrans, CblasNoTrans, _cap_h, _cap_h, w, (T)1.0,
+                              h2hdiff_htm1, hidden_htm1, (T)1.0, h2h_diff);
+                    call_gemm(blas, CblasTrans, CblasNoTrans, _cap_h, _cap_h, w, (T)1.0,
+                              h2hrdiff_htm1, hidden_htm1, (T)1.0, h2hr_diff);
+                    call_gemm(blas, CblasTrans, CblasNoTrans, _cap_h, _cap_h, w, (T)1.0,
+                              h2hzdiff_htm1, hidden_htm1, (T)1.0, h2hz_diff);
+                    auto* embedding_et = embedding + new_offset[i] * _cap_e;
+                    auto* e2hdiff_ht = e2hdiff + new_offset[i] * _cap_h;
+                    auto* e2hrdiff_ht = e2hrdiff + new_offset[i] * _cap_h;
+                    auto* e2hzdiff_ht = e2hzdiff + new_offset[i] * _cap_h;
+                    call_gemm(blas, CblasTrans, CblasNoTrans, _cap_h, _cap_e, w, (T)1.0,
+                              e2hdiff_ht, embedding_et, (T)1.0, e2h_diff);
+                    call_gemm(blas, CblasTrans, CblasNoTrans, _cap_h, _cap_e, w, (T)1.0,
+                              e2hrdiff_ht, embedding_et, (T)1.0, e2hr_diff);
+                    call_gemm(blas, CblasTrans, CblasNoTrans, _cap_h, _cap_e, w, (T)1.0,
+                              e2hzdiff_ht, embedding_et, (T)1.0, e2hz_diff);
+                    sse_eltmul(z + new_offset[i] * _cap_h,
+                               hidden_diff + new_offset[i] * _cap_h,
+                               tmp_buffer + new_offset[i - 1] * _cap_h, _cap_h * w);
+                    // add this with diff from top
+                    sse_eltadd(hidden_diff + new_offset[i - 1] * _cap_h,
+                               tmp_buffer + new_offset[i - 1] * _cap_h,
+                               hidden_diff + new_offset[i - 1] * _cap_h, _cap_h * w);
+                    call_gemm(blas, CblasNoTrans, CblasNoTrans, w, _cap_h, _cap_h, (T)1.0,
+                              h2hdiff_htm1, h2h, (T)1.0,
+                              hidden_diff + new_offset[i - 1] * _cap_h);
+                    call_gemm(blas, CblasNoTrans, CblasNoTrans, w, _cap_h, _cap_h, (T)1.0,
+                              h2hrdiff_htm1, h2hr, (T)1.0,
+                              hidden_diff + new_offset[i - 1] * _cap_h);
+                    call_gemm(blas, CblasNoTrans, CblasNoTrans, w, _cap_h, _cap_h, (T)1.0,
+                              h2hzdiff_htm1, h2hz, (T)1.0,
+                              hidden_diff + new_offset[i - 1] * _cap_h);
+                    // bp embedding diff
+                    auto* embedding_diff_et = embedding_diff + new_offset[i] * _cap_e;
+                    call_gemm(blas, CblasNoTrans, CblasNoTrans, w, _cap_e, _cap_h, (T)1.0,
+                              e2hdiff_ht, e2h, (T)0.0, embedding_diff_et);
+                    call_gemm(blas, CblasNoTrans, CblasNoTrans, w, _cap_e, _cap_h, (T)1.0,
+                              e2hrdiff_ht, e2hr, (T)1.0, embedding_diff_et);
+                    call_gemm(blas, CblasNoTrans, CblasNoTrans, w, _cap_e, _cap_h, (T)1.0,
+                              e2hzdiff_ht, e2hz, (T)1.0, embedding_diff_et);
+                }
+                for (int i = 0; i < batch * _cap_h; i++) {
+                    e2hdiff[i] = (1. - z[i]) * d_tilde[i] * hidden_diff[i];
+                    e2hzdiff[i] = (-tilde[i]) * d_z[i] * hidden_diff[i];
+                }
+                call_gemm(blas, CblasTrans, CblasNoTrans, _cap_h, _cap_e, batch, (T)1.0,
+                          e2hdiff, embedding, (T)1.0, e2h_diff);
+                call_gemm(blas, CblasTrans, CblasNoTrans, _cap_h, _cap_e, batch, (T)1.0,
+                          e2hzdiff, embedding, (T)1.0, e2hz_diff);
+                call_gemm(blas, CblasNoTrans, CblasNoTrans, batch, _cap_e, _cap_h, (T)1.0,
+                          e2hdiff, e2h, (T)0.0, embedding_diff);
+                call_gemm(blas, CblasNoTrans, CblasNoTrans, batch, _cap_e, _cap_h, (T)1.0,
+                          e2hzdiff, e2hz, (T)1.0, embedding_diff);
+                // copy back to original embedding diff, and hidden diff (probablly no use,
+                // but for safety)
+                copy_back(ctx, embedding_diff, ediff, _cap_e);
+                //_layout_helper.copy_back(hidden_diff, top_diff, _cap_h);
+            }
+        };
+    }  // namespace operators
+}  // namespace paddle
+namespace ops = paddle::operators;
+namespace plt = paddle::platform;
+namespace frm = paddle::framework;
+REGISTER_OPERATOR(search_grnn, ops::SearchGrnnOP, ops::SearchGrnnOpMaker,
+        frm::DefaultGradOpMaker<paddle::framework::OpDesc, true>,
+        frm::DefaultGradOpMaker<paddle::imperative::OpBase, true>);
+REGISTER_OPERATOR(search_grnn_grad, ops::SearchGrnnOpGrad);
+REGISTER_OP_CPU_KERNEL(search_grnn,
+        ops::CPUSearchGrnnOPKernel<plt::CPUDeviceContext, float>
+//     ops::CPUSearchGrnnOPKernel<plt::CPUDeviceContext,
+//                                       double>
+);
+REGISTER_OP_CPU_KERNEL(
+        search_grnn_grad,
+        ops::CPUSearchGrnnOPGradKernel<plt::CPUDeviceContext, float>
+//     ops::CPUSearchGrnnOPGradKernel<plt::CPUDeviceContext,
+//                                           double>
+);
\ No newline at end of file
diff --git a/paddle/fluid/operators/search_grnn_op.h b/paddle/fluid/operators/search_grnn_op.h
new file mode 100644
index 0000000000..adcd0d2dad
--- /dev/null
+++ b/paddle/fluid/operators/search_grnn_op.h
@@ -0,0 +1,19 @@
+/* Copyright (c) 2018 PaddlePaddle Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License. */
+#pragma once
+#include "paddle/fluid/framework/op_registry.h"
+namespace paddle {
+    namespace operators {
+        using Tensor = framework::Tensor;
+        using LoDTensor = framework::LoDTensor;
+        using LoD = framework::LoD;
+    }  // namespace operators
+}  // namespace paddle
\ No newline at end of file
diff --git a/paddle/fluid/operators/search_seq_fc_op.cc b/paddle/fluid/operators/search_seq_fc_op.cc
new file mode 100644
index 0000000000..eb7e6de2c0
--- /dev/null
+++ b/paddle/fluid/operators/search_seq_fc_op.cc
@@ -0,0 +1,178 @@
+/* Copyright (c) 2018 PaddlePaddle Authors. All Rights Reserved.
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+    http://www.apache.org/licenses/LICENSE-2.0
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License. */
+#include <cmath>
+#include "paddle/fluid/framework/op_registry.h"
+#include "paddle/fluid/operators/search_compute_2.h"
+namespace paddle {
+namespace operators {
+    using Tensor = framework::Tensor;
+    using LoDTensor = framework::LoDTensor;
+    using LoD = framework::LoD;
+    class SearchSeqFCOpMaker : public framework::OpProtoAndCheckerMaker {
+    public:
+        void Make() override {
+            AddInput("X",
+                     "X (LoDTensor, default LoDTensor<float>) Input variable which "
+                     "should contain lod information.");
+            AddInput("W", "W (Tensor)");
+            AddInput("b", "b (LoDTensor)");
+            AddAttr<int>("out_size", "out_size: the output size")
+                    .SetDefault(0)
+                    .EqualGreaterThan(1);
+            AddAttr<bool>("has_bias", "true or false").SetDefault(true);
+            AddOutput("Out",
+                      "Out (LoDTensor, default LoDTensor<float>) Output variable");
+            AddComment(R"DOC(
+SearchSeqFC
+NOTE: only support 'float32' data type now.
+)DOC");
+        }
+    };
+    class SearchSeqFCOP : public framework::OperatorWithKernel {
+    public:
+        using framework::OperatorWithKernel::OperatorWithKernel;
+        void InferShape(framework::InferShapeContext* ctx) const override {
+            PADDLE_ENFORCE(ctx->HasInput("X"), "X(Input) should not be null.");
+            PADDLE_ENFORCE(ctx->HasInput("W"), "W(Input) should not be null.");
+            PADDLE_ENFORCE(ctx->HasOutput("Out"), "Out(Output) should not be null.");
+            int out_size = ctx->Attrs().Get<int>("out_size");
+            bool has_bias = ctx->Attrs().Get<bool>("has_bias");
+            auto x_dims = ctx->GetInputDim("X");
+            PADDLE_ENFORCE_EQ(x_dims.size(), 2, "The rank of X(Input) should be 2.");
+            auto w_dims = ctx->GetInputDim("W");
+            PADDLE_ENFORCE_EQ(w_dims.size(), 2, "W should be 2-D tensor");
+            PADDLE_ENFORCE_EQ(w_dims[0], out_size,
+                              "wrong shape: w_dims[0] != out_size");
+            PADDLE_ENFORCE_EQ(w_dims[1], x_dims[1],
+                              "wrong shape: w_dims[1] != x_dims[1]");
+            if (has_bias) {
+                PADDLE_ENFORCE(ctx->HasInput("b"), "b(Input) should not be null.");
+                auto b_dims = ctx->GetInputDim("b");
+                PADDLE_ENFORCE_EQ(b_dims.size(), 1, "b should be 1-D tensor");
+            }
+            if (ctx->IsRuntime()) {
+                framework::Variable* x_var =
+                        boost::get<framework::Variable*>(ctx->GetInputVarPtrs("X")[0]);
+                const auto& x_lod = x_var->Get<LoDTensor>().lod();
+                PADDLE_ENFORCE(!x_lod.empty(), "The Input(X) must hold lod info.");
+                const auto& x_lod_0 = x_lod[0];
+                PADDLE_ENFORCE_GE(x_lod_0.size(), 2,
+                                  "The Input(X)'s lod info is corrupted.");
+                PADDLE_ENFORCE_EQ(
+                        x_dims[0], static_cast<int64_t>(x_lod_0.back()),
+                        "The Input(X)'s lod info mismatches the actual tensor shape.");
+            } else {
+                // compile time
+            }
+            ctx->SetOutputDim("Out", framework::make_ddim({-1, out_size}));
+            ctx->ShareLoD("X", /*->*/ "Out");
+        }
+    };
+    template <typename DeviceContext, typename T>
+    class CPUSearchSeqFCOPKernel : public framework::OpKernel<T> {
+    public:
+        void Compute(const framework::ExecutionContext& ctx) const override {
+            auto* bottom = ctx.Input<LoDTensor>("X");
+            auto* w = ctx.Input<Tensor>("W");
+            auto* b = ctx.Input<Tensor>("b");
+            auto* top = ctx.Output<LoDTensor>("Out");
+            bool _bias_term = ctx.Attr<bool>("has_bias");
+            int _out = w->dims()[0];
+            int _in = w->dims()[1];
+            int res_num = bottom->dims()[0];
+            top->Resize(framework::make_ddim({res_num, _out}));
+            const auto* bottom_data = bottom->data<T>();
+            auto* top_data = top->mutable_data<T>(ctx.GetPlace());
+            const auto* weights = w->data<T>();
+            call_gemm(ctx, CblasNoTrans, CblasTrans, res_num, _out, _in, (T)1.0,
+                      bottom_data, weights, (T)0.0, top_data);
+            if (_bias_term) {
+                const auto* bias = b->data<T>();;
+                for (int i = 0; i < res_num; ++i) {
+                    sse_eltadd(top_data + i * _out, bias, top_data + i * _out, _out);
+                }
+            }
+        }
+    };
+    class SearchSeqFCOpGrad : public framework::OperatorWithKernel {
+    public:
+        using framework::OperatorWithKernel::OperatorWithKernel;
+        void InferShape(framework::InferShapeContext* ctx) const override {
+            PADDLE_ENFORCE(ctx->HasInput("X"), "Input(X) should not be null.");
+            PADDLE_ENFORCE(ctx->HasInput("W"), "Input(W) should not be null.");
+            PADDLE_ENFORCE(ctx->HasInput("b"), "Input(b) should not be null.");
+            PADDLE_ENFORCE(ctx->HasInput(framework::GradVarName("Out")),
+                           "Input(Out@GRAD) of SequencePadGradOp should not be null.");
+            PADDLE_ENFORCE(ctx->HasOutput(framework::GradVarName("X")));
+            PADDLE_ENFORCE(ctx->HasOutput(framework::GradVarName("W")));
+            ctx->SetOutputDim(framework::GradVarName("X"), ctx->GetInputDim("X"));
+            ctx->ShareLoD("X", /*->*/ framework::GradVarName("X"));
+            ctx->SetOutputDim(framework::GradVarName("W"), ctx->GetInputDim("W"));
+            bool has_bias = ctx->Attrs().Get<bool>("has_bias");
+            if (has_bias) {
+                PADDLE_ENFORCE(ctx->HasOutput(framework::GradVarName("b")));
+                ctx->SetOutputDim(framework::GradVarName("b"), ctx->GetInputDim("b"));
+            }
+        }
+    };
+    template <typename DeviceContext, typename T>
+    class CPUSearchSeqFCOPGradKernel : public framework::OpKernel<T> {
+    public:
+        void Compute(const framework::ExecutionContext& ctx) const override {
+            auto* bottom = ctx.Input<LoDTensor>("X");
+            auto* w = ctx.Input<Tensor>("W");
+            bool _bias_term = ctx.Attr<bool>("has_bias");
+            int _out = w->dims()[0];
+            int _in = w->dims()[1];
+            auto* d_out = ctx.Input<LoDTensor>(framework::GradVarName("Out"));
+            auto* d_x = ctx.Output<LoDTensor>(framework::GradVarName("X"));
+            auto* d_w = ctx.Output<Tensor>(framework::GradVarName("W"));
+            int res_num = bottom->dims()[0];
+            const auto* top_diff = d_out->data<T>();
+            const auto* bottom_data = bottom->data<T>();
+            auto* bottom_diff = d_x->mutable_data<T>(ctx.GetPlace());
+            const auto* weights = w->data<T>();
+            auto* weights_diff = d_w->mutable_data<T>(ctx.GetPlace());
+            call_gemm(ctx, CblasTrans, CblasNoTrans, _out, _in, res_num, (T)1.0,
+                      top_diff, bottom_data, (T)0.0, weights_diff);
+            call_gemm(ctx, CblasNoTrans, CblasNoTrans, res_num, _in, _out, (T)1.0,
+                      top_diff, weights, (T)0.0, bottom_diff);
+            if (_bias_term) {
+                auto* d_b = ctx.Output<Tensor>(framework::GradVarName("b"));
+                auto* bias_diff = d_b->mutable_data<T>(ctx.GetPlace());
+                memset(bias_diff, (T)0.0, _out * sizeof(T));
+                for (int i = 0; i < res_num; ++i) {
+                    sse_eltadd(bias_diff, top_diff + i * _out, bias_diff, _out);
+                }
+            }
+
+        }
+    };
+}  // namespace operators
+}  // namespace paddle
+namespace ops = paddle::operators;
+namespace plt = paddle::platform;
+namespace frm = paddle::framework;
+REGISTER_OPERATOR(search_seq_fc, ops::SearchSeqFCOP, ops::SearchSeqFCOpMaker,
+                  frm::DefaultGradOpMaker<paddle::framework::OpDesc, true>,
+                  frm::DefaultGradOpMaker<paddle::imperative::OpBase, true>);
+REGISTER_OPERATOR(search_seq_fc_grad, ops::SearchSeqFCOpGrad);
+REGISTER_OP_CPU_KERNEL(search_seq_fc,
+                       ops::CPUSearchSeqFCOPKernel<plt::CPUDeviceContext, float>
+//     ops::CPUSearchSeqFCOPKernel<plt::CPUDeviceContext,
+//                                       double>
+);
+REGISTER_OP_CPU_KERNEL(
+        search_seq_fc_grad,
+        ops::CPUSearchSeqFCOPGradKernel<plt::CPUDeviceContext, float>
+//     ops::CPUSearchSeqFCOPGradKernel<plt::CPUDeviceContext,
+//                                           double>
+);
\ No newline at end of file
diff --git a/python/paddle/fluid/layers/__init__.py b/python/paddle/fluid/layers/__init__.py
index 1d49b91003..f244ccc423 100644
--- a/python/paddle/fluid/layers/__init__.py
+++ b/python/paddle/fluid/layers/__init__.py
@@ -17,6 +17,8 @@ from __future__ import print_function
 from . import ops
 from .ops import *
 from . import nn
+from . import search_nn
+from .search_nn import *
 from .nn import *
 from . import io
 from .io import *
@@ -42,6 +44,7 @@ from . import rnn
 
 __all__ = []
 __all__ += nn.__all__
+__all__ += search_nn.__all__
 __all__ += io.__all__
 __all__ += tensor.__all__
 __all__ += control_flow.__all__
diff --git a/python/paddle/fluid/layers/search_nn.py b/python/paddle/fluid/layers/search_nn.py
new file mode 100644
index 0000000000..9c708c48a4
--- /dev/null
+++ b/python/paddle/fluid/layers/search_nn.py
@@ -0,0 +1,148 @@
+# Copyright (c) 2018 PaddlePaddle Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+"""
+All layers just related to the neural network.
+"""
+
+from __future__ import print_function
+
+from ..layer_helper import LayerHelper
+
+__all__ = [
+    'search_fc',
+    'search_seq_fc',
+    'search_grnn',
+    ]
+
+
+def search_fc(
+        input,
+        size,
+        param_attr=None,
+        bias_attr=None,
+        act=None,
+        is_test=False,
+        name=None):
+    """
+
+    TODO:
+    """
+    helper = LayerHelper('search_fc', **locals())
+    dtype = input.dtype
+    input_shape = list(input.shape)
+    assert len(input_shape) == 2
+    w_shape = [size, input_shape[1]]
+    w = helper.create_parameter(attr=param_attr, shape=w_shape, dtype=dtype, is_bias=False)
+    b_shape = [size]
+    b = helper.create_parameter(attr=bias_attr, shape=b_shape, dtype=dtype, is_bias=False)
+    res = helper.create_variable_for_type_inference(dtype)
+    helper.append_op(
+        type='search_fc',
+        inputs={
+            'X': input,
+            'W': w,
+            'b': b,
+        },
+        outputs={"Out": res, },
+        attrs={'out_size': size, }
+    )
+
+    return res
+
+
+def search_seq_fc(
+        input,
+        size,
+        param_attr=None,
+        bias_attr=None,
+        act=None,
+        is_test=False,
+        name=None):
+    """
+
+    TODO:
+    """
+    helper = LayerHelper('search_seq_fc', **locals())
+    dtype = input.dtype
+    input_shape = list(input.shape)
+    assert len(input_shape) == 2
+    w_shape = [size, input_shape[1]]
+    w = helper.create_parameter(attr=param_attr, shape=w_shape, dtype=dtype, is_bias=False)
+    input_dict = {'X': input, 'W': w,}
+    has_bias = False
+    if bias_attr is not None:
+        b_shape = [size]
+        b = helper.create_parameter(attr=bias_attr, shape=b_shape, dtype=dtype, is_bias=False)
+        input_dict['b'] = b
+        has_bias = True
+    res = helper.create_variable_for_type_inference(dtype)
+    helper.append_op(
+        type='search_seq_fc',
+        inputs=input_dict,
+        outputs={"Out": res, },
+        attrs={'out_size': size, 'has_bias': has_bias}
+    )
+
+    return res
+
+
+def search_grnn(
+        input,
+        num_input,
+        num_hidden,
+        param_attr_in,
+        param_attr_hidden,
+        dtype='float32',
+        is_test=False,
+        name=None):
+    """
+
+    TODO:
+    """
+
+    helper = LayerHelper('search_grnn', **locals())
+
+    input_shape = list(input.shape)
+    assert len(input_shape) == 2 and input_shape[-1] == num_input
+
+    _cap_h = num_hidden
+    _cap_e = input_shape[-1]
+    wi_shape = [3, _cap_h, _cap_e]
+    wh_shape = [3, _cap_h, _cap_h]
+    wi = helper.create_parameter(
+        attr=param_attr_in, shape=wi_shape, dtype=dtype, is_bias=False)
+    wh = helper.create_parameter(
+        attr=param_attr_hidden, shape=wh_shape, dtype=dtype, is_bias=False)
+
+    grnn_res = helper.create_variable_for_type_inference(dtype)
+    grnn_buffer = helper.create_variable_for_type_inference(dtype)
+    grnn_idx_sorted_by_width = helper.create_variable_for_type_inference(dtype)
+    grnn_layout_input = helper.create_variable_for_type_inference(dtype)
+
+    helper.append_op(
+        type='search_grnn',
+        inputs={
+            'X': input,
+            'Wi': wi,
+            'Wh': wh,
+        },
+        outputs={"Out": grnn_res,
+                 "tmp_buffer": grnn_buffer,
+                 'idx_sorted_by_width': grnn_idx_sorted_by_width,
+                 'layout_input': grnn_layout_input
+                 },
+        attrs={'num_input': num_input, 'num_hidden': num_hidden}
+    )
+
+    return grnn_res
-- 
2.20.1 (Apple Git-117)

